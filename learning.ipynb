{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "Python 3.8.2 64-bit",
   "display_name": "Python 3.8.2 64-bit",
   "metadata": {
    "interpreter": {
     "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sklearn\n",
    "from sklearn import neighbors\n",
    "from sklearn import tree\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GroupKFold\n",
    "from sklearn.metrics import mean_squared_error\n"
   ]
  },
  {
   "source": [
    "Let's start by importing our data for the region & time split strategies."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "hospPredictions = np.genfromtxt('combined.csv', delimiter=',', skip_header=True, usecols = (123))\n",
    "features = np.genfromtxt('combined.csv', delimiter=',', skip_header=True, usecols = (range(123)))[:, 2:]\n",
    "regions = np.genfromtxt('combined.csv', delimiter=',', skip_header=True, usecols = (0), dtype = 'str')\n",
    "dates = np.genfromtxt('combined.csv', delimiter=',', skip_header=True, usecols = (1), dtype = 'datetime64')"
   ]
  },
  {
   "source": [
    "With that out of the way, we can begin by splitting data based on the time. The selected time for the split is'2020-08-10'. Any dates after this will be in the test set and the rest of the data will be used for training. We will then test it with KNN/Decision trees."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "MSE (KNN):           3980.4\nMSE (DecisionTree):  4622.9\n"
     ]
    }
   ],
   "source": [
    "# get the indices for train/test data\n",
    "trainIndicesDate = np.argwhere(dates > np.datetime64(\"2020-08-10\"))[:,0]\n",
    "testIndicesDate = np.argwhere(dates <= np.datetime64(\"2020-08-10\"))[:,0]\n",
    "\n",
    "# prep the dataset\n",
    "xTrain = np.take(features, trainIndicesDate, axis = 0)\n",
    "xTest = np.take(features, testIndicesDate, axis = 0)\n",
    "yTrain = np.take(hospPredictions, trainIndicesDate, axis = 0)\n",
    "yTest = np.take(hospPredictions, testIndicesDate, axis = 0)\n",
    "\n",
    "# KNN - This was used to determine the number of neighbours\n",
    "#bestK = -1\n",
    "#bestError = sys.maxsize\n",
    "#for i in range(1, 36):\n",
    "    #newError = mean_squared_error(yTest, predictionKnn)\n",
    "    #if (newError < bestError):\n",
    "        #bestError = newError\n",
    "        #bestK = i\n",
    "\n",
    "#print(\"best K is\", bestK)\n",
    "#print(\"error is\", bestError)\n",
    "\n",
    "# tested to be the best the optimal numebr of neighbors\n",
    "knn = neighbors.KNeighborsRegressor(n_neighbors=34)\n",
    "knn.fit(xTrain, yTrain)\n",
    "predictionKnn = knn.predict(xTest)\n",
    "\n",
    "# decision trees\n",
    "# note: random state set to improve reproducibility \n",
    "clf = tree.DecisionTreeRegressor(random_state=0)\n",
    "clf = clf.fit(xTrain, yTrain)\n",
    "predictionTree = clf.predict(xTest)\n",
    "\n",
    "# get the mean squared error in order to verify the results\n",
    "print(\"MSE (KNN):          %7.1f\" % (mean_squared_error(yTest, predictionKnn)))\n",
    "print(\"MSE (DecisionTree): %7.1f\" % (mean_squared_error(yTest, predictionTree)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "source": [
    "We can now move onto a split by region.\n",
    "\n",
    "Note that because the required split is such that an entire region belongs to a training or testing set, it is not possible to achieve an 80-20 split in our dataset. Instead, 75-25 is achieved. \n",
    "\n",
    "This is because our dataset consists of 640 instances - 40 datapoints per region for a total of 16 regions.\n",
    "As a result, an 80-20 split results in a train-test split of 512-128 which means that a region will occur in both testing and training which is a contradiction of requirements. "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Mean MSE (KNN):           3870.8\nMean MSE (DecisionTree):  5835.0\n"
     ]
    }
   ],
   "source": [
    "regionScoresKnn = []\n",
    "regionScoresTree = []\n",
    " \n",
    "#gss = GroupShuffleSplit(n_splits=5, train_size=0.8)\n",
    "gkf = GroupKFold(n_splits=5)\n",
    "\n",
    "for train_idx, test_idx in gkf.split(features, hospPredictions, regions):\n",
    "    trainIndices = np.array(train_idx)\n",
    "    testIndices = np.array(test_idx)\n",
    "\n",
    "    xTrain = np.take(features, trainIndices, axis = 0)\n",
    "    xTest = np.take(features, testIndices, axis = 0)\n",
    "    yTrain = np.take(hospPredictions, trainIndices, axis = 0)\n",
    "    yTest = np.take(hospPredictions, testIndices, axis = 0)\n",
    "\n",
    "    # to determine the optimal # of neighbours here, the same logic was applied as before but this was averaged for each fold and then manually tuned\n",
    "    knn = neighbors.KNeighborsRegressor(n_neighbors=44)\n",
    "    knn.fit(xTrain, yTrain)\n",
    "    predictionKnn = knn.predict(xTest)\n",
    "\n",
    "    clf = tree.DecisionTreeRegressor(random_state=0)\n",
    "    clf = clf.fit(xTrain, yTrain)\n",
    "    predictionTree = clf.predict(xTest)\n",
    "\n",
    "    regionScoresKnn.append(mean_squared_error(yTest, predictionKnn))\n",
    "    regionScoresTree.append(mean_squared_error(yTest, predictionTree))\n",
    "\n",
    "print(\"Mean MSE (KNN):          %7.1f\" % (np.mean(regionScoresKnn)))\n",
    "print(\"Mean MSE (DecisionTree): %7.1f\" % (np.mean(regionScoresTree)))"
   ]
  },
  {
   "source": [
    "For additional testing, we will model each region separately.\n",
    "\n",
    "Once again, we will use both KNNs and decision trees."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Region: US-AK | MSE (KNN):   139.6 | MSE (DecisionTree):   212.2\nRegion: US-DC | MSE (KNN):     0.0 | MSE (DecisionTree):     0.0\nRegion: US-DE | MSE (KNN):     0.0 | MSE (DecisionTree):     0.0\nRegion: US-HI | MSE (KNN):   131.1 | MSE (DecisionTree):   670.8\nRegion: US-ID | MSE (KNN):  2650.3 | MSE (DecisionTree):  1765.6\nRegion: US-ME | MSE (KNN):   107.6 | MSE (DecisionTree):   569.9\nRegion: US-MT | MSE (KNN):   194.2 | MSE (DecisionTree):   526.4\nRegion: US-ND | MSE (KNN):    28.9 | MSE (DecisionTree):   814.5\nRegion: US-NE | MSE (KNN): 85666.6 | MSE (DecisionTree): 80070.1\nRegion: US-NH | MSE (KNN):  1033.4 | MSE (DecisionTree):  1478.5\nRegion: US-NM | MSE (KNN):  8197.6 | MSE (DecisionTree):  9087.5\nRegion: US-RI | MSE (KNN):  4570.2 | MSE (DecisionTree):  6680.6\nRegion: US-SD | MSE (KNN):    47.8 | MSE (DecisionTree):    84.8\nRegion: US-VT | MSE (KNN):   126.6 | MSE (DecisionTree):   182.2\nRegion: US-WV | MSE (KNN):  3362.1 | MSE (DecisionTree):  6724.1\nRegion: US-WY | MSE (KNN):    48.5 | MSE (DecisionTree):    98.7\n"
     ]
    }
   ],
   "source": [
    "combined = np.genfromtxt('combined.csv', delimiter=',', skip_header=True, usecols = (range(124)), dtype = 'str') # load the entire dataset\n",
    "combined = np.delete(combined, 1, 1) # remove the dates \n",
    "regionSplit = np.split(combined[:, :], np.cumsum(np.unique(combined[:, 0], return_counts=True)[1])[:-1]) # create a list of arrays that contains the features/predictions for a specific region\n",
    "\n",
    "for regionalData in regionSplit:\n",
    "    currentRegion = regionalData[0,0] # extract the current region we're predicting\n",
    "    x = regionalData[:, 1:-1].astype(np.float) # features for the specific region\n",
    "    y = regionalData[:,-1].astype(np.float) # predictions for each instance of that region\n",
    "\n",
    "    # here, we will do a simple KNN (k=2) with an 80-20 split\n",
    "    # this case is a bit more complex to select an optimal number of neighbours so it was manually tuned\n",
    "    xTrain, xTest, yTrain, yTest = train_test_split(x, y, train_size = 0.8, random_state = 0)\n",
    "    knn = neighbors.KNeighborsRegressor(n_neighbors=2)\n",
    "    knn.fit(xTrain, yTrain)\n",
    "    predictionKnn = knn.predict(xTest)\n",
    "\n",
    "    clf = tree.DecisionTreeRegressor(random_state=0)\n",
    "    clf = clf.fit(xTrain, yTrain)\n",
    "    predictionTree = clf.predict(xTest)\n",
    "\n",
    "    print(\"Region: %s | MSE (KNN): %7.1f | MSE (DecisionTree): %7.1f\" % (currentRegion, mean_squared_error(yTest, predictionKnn), mean_squared_error(yTest, predictionTree)))"
   ]
  }
 ]
}